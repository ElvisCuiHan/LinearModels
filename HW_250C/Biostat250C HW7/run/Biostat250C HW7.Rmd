---
title: "Biostat250C HW7"
author: | 
  | Elvis Cui
  | Biostatistics Department, UCLA
date: "May, $26^{th}$, 2021"
insititution: "UCLA"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("~/Desktop/ElvisCui/UCLA_Study/Bio250C/HW/Biostat250CHW7/run/")
rm(list=ls())
source("../src/BayesianLM.R")
library(MCMCpack)
library(MASS)
library(tidyverse)
```

## Question 1

- Split the whole dataset into $K$ subsets such that each subset contains $m_k$ points and $\max_{1\le k\le K}m_k\le 200$.

## Sol Q1

- First, use the `random_split_datasets` function that I have written in advance to generate $K$ datasets and store them as `.txt` files.

```{r}
# Load data
K = 10
# The following step has already been done.

#data = read.table("../data/LinearModelExample.txt", header = T)
#random_split_datasets(data=data, K=K, path = "../data")
```

## Question 2

- How can you take the output from `lm()` function and integrate that into your Bayes updating function?
    
## Sol Q2

- First, fit a classical linear model.
- Second, update parameters using the newly written function.
- I call `model.matrix()` and `model.frame()` function to access $X_k$ and $y_k$ in the `mod` subject.

```{r}
for (k in 1:K) {
   # I: Read datasets
   data_k = read.table(paste0("../data/subset", k, ".txt"), header = T)
  
   # II: set hyper-parameters
   # number of covariates including intercept.
   p = dim(data_k)[2]
   # number of obs
   n = dim(data_k)[1]
   # design matrix and response vector.
   X0 = as.matrix(data_k[, -1])
   y = data_k$Y
   classical_mod = lm(y~X0)
  
   if (k==1) {
     # prior for beta.
     mu_beta = rep(1, p)
     inv_V_beta = 0
     # prior for sigma^2.
     a = - p / 2; b = 0 
   }
   
   # III: Let's calculate posterior parameters !!!!!!
   out = NIG_pos_para_lm(mod=classical_mod, mu_beta=mu_beta, inv_V_beta=inv_V_beta, a=a,b=b)
   
   # IV: Let's update hyper-parameters !!!!!
   # KEY STEP in Bayesian statistics.
   mu_beta = out[[1]]
   inv_V_beta = out[[2]]
   a = out[[3]]
   b = out[[4]]
}
print("Mu_Beta is:")
print(round(mu_beta, 3))
print("Inverse of V_beta is:")
print(round(inv_V_beta, 1))
print("a and b are:")
print(a)
print(b)
sequential_method = mu_beta
```

## Question 3

- Verify your results exactly match with previous HW.

## Sol Q3

- Recover the results from composition sampling.

```{r, include=FALSE}
data = read.table("../data/LinearModelExample.txt", header = T)
# number of covariates including intercept.
p = dim(data)[2]
# number of obs
n = dim(data)[1]
# design matrix and response vector.
X0 = as.matrix(data[, -1])
X = cbind(1, X0)
y = data$Y
# prior for beta.
mu_beta = rep(1, p)
inv_V_beta = 0
# prior for sigma^2.
a = - p / 2; b = 0
```
```{r}
cs = rposNIG_comp(n=100, y=y, X=X, mu_beta=mu_beta, inv_V_y = diag(length(y)), inv_V_beta=inv_V_beta, a=a,b=b)
```

- I will compare the (posterior) mean of $p$-dimensional vector $\beta$ from composition sampling and the sequential methods.

```{r}

comp_sampling = apply(cs[, -(p+1)], 2, mean)
tibble(comp_sampling, sequential_method)
```
- The slight differences are due to
 1. Floating points error by the sequential method.
 2. Randomness from the composition sampling method.
